{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lily/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sequential library pyhton keras libaray\n",
    "#the sequential model is for creating model layer by layer \n",
    "from keras.models import Sequential\n",
    "#import dense layers fuction for creating neurons in layer\n",
    "from keras.layers import Dense\n",
    "#import numpy library to add high-level mathematical functions to operate on datas  \n",
    "import numpy\n",
    "#The random module uses the seed value as a base to generate a random number.\n",
    "#Seeding a pseudo-random number generator gives it its first \"previous\" value.\n",
    "#That is, if you have the same seed, you will get the same sequence of numbers twice.\n",
    "numpy.random.seed(7)\n",
    "#load the csv, split string with \",\"\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "#assigned columns one to eight to x as input\n",
    "#assigned column nine to y as output\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a sequence of layers\n",
    "model = Sequential()\n",
    "#8 input parameters, with 12 neurons in the FIRST hidden layer, the Rectified Linear Unit- Relu\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "#8 neurons in the second hidden layer, the Rectified Linear Unit- Relu\n",
    "model.add(Dense(8, activation='relu'))\n",
    "#the output 1/0\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complie model\n",
    "#loss means cross entropy between the network output and target\n",
    "#Adam Optimization Algorithm \n",
    "#A metric is a function that is used to judge the performance of your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s 436us/step - loss: 3.7110 - acc: 0.5990\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.9366 - acc: 0.5924\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.7472 - acc: 0.6406\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.7115 - acc: 0.6576\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.6823 - acc: 0.6745\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.6511 - acc: 0.6849\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.6495 - acc: 0.6758\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.6369 - acc: 0.6836\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.6246 - acc: 0.6979\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.6293 - acc: 0.6771\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.6472 - acc: 0.6758\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.6385 - acc: 0.6719\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.6250 - acc: 0.6771\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.6171 - acc: 0.7018\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.6016 - acc: 0.6953\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.5880 - acc: 0.7005\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.5834 - acc: 0.6992\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 120us/step - loss: 0.6001 - acc: 0.6862\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.5797 - acc: 0.7135\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.5790 - acc: 0.7240\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5703 - acc: 0.7135\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5823 - acc: 0.6966\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.5748 - acc: 0.7135\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5685 - acc: 0.7292\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.5583 - acc: 0.7357\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.5705 - acc: 0.7070\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5554 - acc: 0.7240\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.5554 - acc: 0.7292\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.5739 - acc: 0.7148\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5609 - acc: 0.7201\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.5684 - acc: 0.7188\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5652 - acc: 0.7148\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5539 - acc: 0.7148\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 150us/step - loss: 0.5517 - acc: 0.7279\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5544 - acc: 0.7122\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5617 - acc: 0.7083\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5354 - acc: 0.7344\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5453 - acc: 0.7174\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5496 - acc: 0.7253\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5489 - acc: 0.7188\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5462 - acc: 0.7318\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5379 - acc: 0.7435\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5338 - acc: 0.7435\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 134us/step - loss: 0.5346 - acc: 0.7448\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5346 - acc: 0.7526\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.5305 - acc: 0.7591\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5350 - acc: 0.7357\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.5348 - acc: 0.7370\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5352 - acc: 0.7500\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.5275 - acc: 0.7409\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5292 - acc: 0.7448\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5312 - acc: 0.7435\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5377 - acc: 0.7500\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.5377 - acc: 0.7331\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.5227 - acc: 0.7513\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5285 - acc: 0.7448\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.5313 - acc: 0.7357\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.5237 - acc: 0.7500\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5132 - acc: 0.7617\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5352 - acc: 0.7370\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5259 - acc: 0.7383\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.5171 - acc: 0.7565\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.5461 - acc: 0.7344\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.5314 - acc: 0.7422\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5231 - acc: 0.7422\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5076 - acc: 0.7487\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.5164 - acc: 0.7383\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.5144 - acc: 0.7526\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5148 - acc: 0.7552\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.5387 - acc: 0.7188\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5180 - acc: 0.7370\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5170 - acc: 0.7435\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5168 - acc: 0.7461\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5105 - acc: 0.7591\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.5095 - acc: 0.7578\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5134 - acc: 0.7539\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.5165 - acc: 0.7604\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.5149 - acc: 0.7526\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.5139 - acc: 0.7409\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5104 - acc: 0.7565\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5059 - acc: 0.7656\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.5047 - acc: 0.7552\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5012 - acc: 0.7604\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.4991 - acc: 0.7513\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5064 - acc: 0.7422\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5064 - acc: 0.7539\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.4995 - acc: 0.7565\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5013 - acc: 0.7656\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5049 - acc: 0.7656\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5111 - acc: 0.7513\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.5019 - acc: 0.7487\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.5114 - acc: 0.7461\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5019 - acc: 0.7630\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.4965 - acc: 0.7695\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 142us/step - loss: 0.5052 - acc: 0.7448\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4917 - acc: 0.7617\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.4998 - acc: 0.7747\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4903 - acc: 0.7656\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 135us/step - loss: 0.4906 - acc: 0.7708\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.4841 - acc: 0.7799\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.4887 - acc: 0.7760\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.4985 - acc: 0.7591\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 127us/step - loss: 0.4986 - acc: 0.7578\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.4911 - acc: 0.7917\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 134us/step - loss: 0.5300 - acc: 0.7448\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.4900 - acc: 0.7708\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 130us/step - loss: 0.4888 - acc: 0.7799\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.4969 - acc: 0.7734\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 132us/step - loss: 0.4875 - acc: 0.7682\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 134us/step - loss: 0.4898 - acc: 0.7747\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.4840 - acc: 0.7786\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.4915 - acc: 0.7760\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.4942 - acc: 0.7630\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.4912 - acc: 0.7630\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.4913 - acc: 0.7786\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.4911 - acc: 0.7760\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 157us/step - loss: 0.4895 - acc: 0.7656\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 133us/step - loss: 0.4886 - acc: 0.7812\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 143us/step - loss: 0.4826 - acc: 0.7656\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.4932 - acc: 0.7734\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4914 - acc: 0.7760\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4864 - acc: 0.7826\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4807 - acc: 0.7695\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.4859 - acc: 0.7760\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.4870 - acc: 0.7826\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.4797 - acc: 0.7812\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.4895 - acc: 0.7708\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.4722 - acc: 0.7773\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.4801 - acc: 0.7695\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.4734 - acc: 0.7930\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.4808 - acc: 0.7695\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4806 - acc: 0.7826\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4824 - acc: 0.7734\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.4846 - acc: 0.7721\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.4765 - acc: 0.7734\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.4733 - acc: 0.7799\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.4672 - acc: 0.7799\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.4805 - acc: 0.7839\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.4649 - acc: 0.7891\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.4821 - acc: 0.7865\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.4737 - acc: 0.7812\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.4836 - acc: 0.7734\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.4753 - acc: 0.7708\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.4760 - acc: 0.7734\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.4880 - acc: 0.7630\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4925 - acc: 0.7695\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4832 - acc: 0.7839\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.4714 - acc: 0.7773\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.4740 - acc: 0.7721\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.4753 - acc: 0.7773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb21ad2518>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the profermance of model\n",
    "#The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire dataset.\n",
    "#The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 56us/step\n",
      "\n",
      "acc: 79.56%\n"
     ]
    }
   ],
   "source": [
    "#evaluate the profermance of model\n",
    "#The average and standard deviation of the model performance is then printed at the end of the run to provide a robust estimate of model accuracy.\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lily/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/lily/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Import pandas for using common data science methods\n",
    "import pandas as pd\n",
    "# Import train_test_split from sklearn library\n",
    "# Read the dataset from csv fiel using pandas library\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "#import dense layers fuction for creating neurons in layer\n",
    "from keras.layers import Dense \n",
    "dataset = pd.read_csv('train_validation.csv',encoding = \"ISO-8859-1\")\n",
    "Y=dataset.iloc[:,0]\n",
    "X=dataset.iloc[:,1]\n",
    "Y = Y.as_matrix(columns=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              sad APL friend.............\n",
      "1                             I missed New Moon trailer...\n",
      "2                                      omg already 7:30 :O\n",
      "3        .. Omgaga. Im sooo im gunna CRy. I've dentist ...\n",
      "4                           think mi bf cheating me!!! T_T\n",
      "5                                              worry much?\n",
      "6                       Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
      "7                 Sunny Again Work Tomorrow :-| TV Tonight\n",
      "8                      handed uniform today . miss already\n",
      "9                              hmmmm.... wonder number @-)\n",
      "10                                 I must think positive..\n",
      "11                         thanks haters face day! 112-102\n",
      "12                                      weekend sucked far\n",
      "13                         jb isnt showing australia more!\n",
      "14                                           ok thats win.\n",
      "15                 &lt;-------- This way feel right now...\n",
      "16       awhhe man.... I'm completely useless rt now. F...\n",
      "17       Feeling strangely fine. Now I'm gonna go liste...\n",
      "18                    HUGE roll thunder now...SO scary!!!!\n",
      "19       I cut beard off. It's growing well year. I'm g...\n",
      "20                                          Very sad Iran.\n",
      "21                                           wompppp wompp\n",
      "22       You're one see cause one else following pretty...\n",
      "23       &lt;---Sad level 3. I writing massive blog twe...\n",
      "24       ... Headed Hospitol : Had pull Golf Tourny 3rd...\n",
      "25       BoRinG ): whats wrong him?? Please tell me.......\n",
      "26       can't bothered. wish could spend rest life sat...\n",
      "27       Feeeling like shit right now. I really want sl...\n",
      "28                     goodbye exams HELLO ALCOHOL TONIGHT\n",
      "29       I realize THAT deep. Geez give girl warning at...\n",
      "                               ...                        \n",
      "99959        @CT415 @UCLA_Bruin made sad too! means albums\n",
      "99960                  @CT415 I agree. I think fetish too!\n",
      "99961    @ct415 I hope serious injury! I'm worried! Tak...\n",
      "99962    @ctabita consolation, weekend isnt quite expec...\n",
      "99963                                @ctayah got back, yo!\n",
      "99964           @ctaylor0127 I can't wait see movie. Enjoy\n",
      "99965    @ctaylor10127 @smelby I excited little nervous...\n",
      "99966    @ctb1221 yeah sorry.going concert night.non re...\n",
      "99967    @ctcash @buildingateam @diabetescure @chocolat...\n",
      "99968    @ctdesign87 im glad went china town actually t...\n",
      "99969                                    @CTerry1985 Sorry\n",
      "99970                        @CTerry1985 damn it, dont sky\n",
      "99971    @CTerry1985 That's thing; new raft Star Wars f...\n",
      "99972                                             @cthagod\n",
      "99973                                 @ctham #FollowFriday\n",
      "99974    @ctham #awaresg You wrong. But male point view...\n",
      "99975    @ctham @mommyfizz cuz big burly man. hahahahah...\n",
      "99976    @ctham @Wilsurn Trying get wider range shirts ...\n",
      "99977                   @ctham Haha I love passion support\n",
      "99978    @cthulhullahoop That sucks...I like living Coo...\n",
      "99979          @cunningstunts till go home till saturday x\n",
      "99980                @cunningstunts22 afternoon jim hows x\n",
      "99981    @cup_a_tea The foot really bad. Like worst eve...\n",
      "99982    @Cup_Of_Katy Have fun health &amp; safety :S J...\n",
      "99983    @cupati It took waaay long get message ashamed...\n",
      "99984    @Cupcake seems like repeating problem hope abl...\n",
      "99985    @cupcake__ arrrr replied different tweets time...\n",
      "99986                             @CuPcAkE_2120 ya thought\n",
      "99987           @Cupcake_Dollie Yes. Yes. I'm glad fun me.\n",
      "99988                              @cupcake_kayla haha yes\n",
      "Name: tweet_without_stopwords, Length: 99989, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#preprocessing data\n",
    "from nltk.corpus import stopwords\n",
    "stopwo = stopwords.words('english')\n",
    "test = pd.DataFrame(X)\n",
    "test.columns = [\"tweet\"]\n",
    "test['tweet_without_stopwords'] = test['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwo)]))\n",
    "print(test['tweet_without_stopwords'])\n",
    "X=test['tweet_without_stopwords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 1000)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#convert text to features\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=1000)\n",
    "# encode document\n",
    "vector = vectorizer.transform(X)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split traing data and test data by the proportion of 8:2 \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a sequence of layers\n",
    "model = Sequential()\n",
    "#there are four layers in the model, 1000 features\n",
    "model.add(Dense(12, input_dim=1000, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "#the output 1/0\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complie model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "79991/79991 [==============================] - 11s 136us/step - loss: 0.5976 - acc: 0.6713\n",
      "Epoch 2/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.5644 - acc: 0.7035\n",
      "Epoch 3/50\n",
      "79991/79991 [==============================] - 10s 121us/step - loss: 0.5478 - acc: 0.7195\n",
      "Epoch 4/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.5330 - acc: 0.7337\n",
      "Epoch 5/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.5199 - acc: 0.7461\n",
      "Epoch 6/50\n",
      "79991/79991 [==============================] - 10s 121us/step - loss: 0.5084 - acc: 0.7563\n",
      "Epoch 7/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.4992 - acc: 0.7654\n",
      "Epoch 8/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4908 - acc: 0.7717\n",
      "Epoch 9/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4832 - acc: 0.7792\n",
      "Epoch 10/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4763 - acc: 0.7845\n",
      "Epoch 11/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4700 - acc: 0.7901\n",
      "Epoch 12/50\n",
      "79991/79991 [==============================] - 10s 125us/step - loss: 0.4646 - acc: 0.7935\n",
      "Epoch 13/50\n",
      "79991/79991 [==============================] - 10s 124us/step - loss: 0.4600 - acc: 0.7970\n",
      "Epoch 14/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.4553 - acc: 0.8005\n",
      "Epoch 15/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.4510 - acc: 0.8036\n",
      "Epoch 16/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.4466 - acc: 0.8074\n",
      "Epoch 17/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.4438 - acc: 0.8079\n",
      "Epoch 18/50\n",
      "79991/79991 [==============================] - 10s 126us/step - loss: 0.4395 - acc: 0.8116\n",
      "Epoch 19/50\n",
      "79991/79991 [==============================] - 11s 133us/step - loss: 0.4355 - acc: 0.8137\n",
      "Epoch 20/50\n",
      "79991/79991 [==============================] - 10s 129us/step - loss: 0.4315 - acc: 0.8163\n",
      "Epoch 21/50\n",
      "79991/79991 [==============================] - 10s 127us/step - loss: 0.4289 - acc: 0.8176\n",
      "Epoch 22/50\n",
      "79991/79991 [==============================] - 10s 129us/step - loss: 0.4261 - acc: 0.81840s - loss: 0.4257 - acc:\n",
      "Epoch 23/50\n",
      "79991/79991 [==============================] - 10s 126us/step - loss: 0.4225 - acc: 0.8216\n",
      "Epoch 24/50\n",
      "79991/79991 [==============================] - 10s 124us/step - loss: 0.4208 - acc: 0.8216\n",
      "Epoch 25/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4176 - acc: 0.82330s - loss: 0.4166 - \n",
      "Epoch 26/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4137 - acc: 0.82470s - loss: 0.4120\n",
      "Epoch 27/50\n",
      "79991/79991 [==============================] - 10s 121us/step - loss: 0.4115 - acc: 0.8255\n",
      "Epoch 28/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4085 - acc: 0.8276\n",
      "Epoch 29/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.4072 - acc: 0.8280\n",
      "Epoch 30/50\n",
      "79991/79991 [==============================] - 10s 128us/step - loss: 0.4049 - acc: 0.8286\n",
      "Epoch 31/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.4023 - acc: 0.8294\n",
      "Epoch 32/50\n",
      "79991/79991 [==============================] - 10s 122us/step - loss: 0.3995 - acc: 0.8307\n",
      "Epoch 33/50\n",
      "79991/79991 [==============================] - 10s 121us/step - loss: 0.3971 - acc: 0.8314\n",
      "Epoch 34/50\n",
      "79991/79991 [==============================] - 10s 131us/step - loss: 0.3959 - acc: 0.8313\n",
      "Epoch 35/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.3935 - acc: 0.8324\n",
      "Epoch 36/50\n",
      "79991/79991 [==============================] - 11s 135us/step - loss: 0.3917 - acc: 0.8329\n",
      "Epoch 37/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.3904 - acc: 0.83262s - ETA: 1s -\n",
      "Epoch 38/50\n",
      "79991/79991 [==============================] - 11s 139us/step - loss: 0.3885 - acc: 0.83371s - \n",
      "Epoch 39/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.3861 - acc: 0.8352\n",
      "Epoch 40/50\n",
      "79991/79991 [==============================] - 103s 1ms/step - loss: 0.3855 - acc: 0.8353\n",
      "Epoch 41/50\n",
      "79991/79991 [==============================] - 11s 143us/step - loss: 0.3829 - acc: 0.8366\n",
      "Epoch 42/50\n",
      "79991/79991 [==============================] - 11s 134us/step - loss: 0.3824 - acc: 0.8365\n",
      "Epoch 43/50\n",
      "79991/79991 [==============================] - 11s 138us/step - loss: 0.3806 - acc: 0.8373\n",
      "Epoch 44/50\n",
      "79991/79991 [==============================] - 11s 135us/step - loss: 0.3792 - acc: 0.8375\n",
      "Epoch 45/50\n",
      "79991/79991 [==============================] - 11s 136us/step - loss: 0.3782 - acc: 0.8383\n",
      "Epoch 46/50\n",
      "79991/79991 [==============================] - 11s 134us/step - loss: 0.3764 - acc: 0.8383\n",
      "Epoch 47/50\n",
      "79991/79991 [==============================] - 10s 125us/step - loss: 0.3759 - acc: 0.8388\n",
      "Epoch 48/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.3741 - acc: 0.8390\n",
      "Epoch 49/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.3733 - acc: 0.8397\n",
      "Epoch 50/50\n",
      "79991/79991 [==============================] - 10s 123us/step - loss: 0.3720 - acc: 0.8404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1b8d59b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the profermance of model\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79991/79991 [==============================] - 1s 16us/step\n",
      "\n",
      "acc: 85.34%\n"
     ]
    }
   ],
   "source": [
    "#the accuracy of train data\n",
    "scores = model.evaluate(X_train, Y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19989/19989 [==============================] - 0s 22us/step\n",
      "\n",
      "acc: 81.00%\n"
     ]
    }
   ],
   "source": [
    "#the accuracy of test data\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I used numpy and pandas to upload csv file into jupter notebook.\n",
    "Second, I used NLTK to remove \"stopwords\" from sentiment text. \n",
    "Third, I transfer words into 500 features.\n",
    "Forth, I built a binary classification model with 500 inputs, 4 hidden layers and each layers have 12 neurals. the activation is rectified linear unit(ReLU) \n",
    "The output is limited to 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the accuracy of train data is 85.34%\n",
    "the accuracy of test data is 81.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
